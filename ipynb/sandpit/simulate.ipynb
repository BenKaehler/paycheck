{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n",
    "do not run - takes hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "redbiom search metadata 'where sample_type == \"stool\"' > stool_samples\n",
    "redbiom search metadata 'where sample_type == \"Stool\"' >> stool_samples\n",
    "export CTX=Deblur-illumina-16S-v4-150nt-10d7e0\n",
    "redbiom fetch samples --from stool_samples --context $CTX --output stool_sv.biom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "do not run - takes overnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime tools import --input-path 99_otus.fasta --output-path 99_otus.qza --type FeatureData[Sequence]\n",
    "qiime feature-classifier extract-reads --i-sequences 99_otus.qza --p-f-primer GTGYCAGCMGCCGCGGTAA --p-r-primer GGACTACNVGGGTWTCTAAT --o-reads 99_otus_v4.qza\n",
    "qiime tools export 99_otus_v4.qza --output-dir .\n",
    "mv dna-sequences.fasta 99_otus_v4.fasta\n",
    "biom table-ids --observations -i stool_sv.biom | awk '{print \">\"$1\"blast_rocks\\n\"$1}' > stool_sv.fasta\n",
    "makeblastdb -in 99_otus_v4.fasta -dbtype nucl -out 99_otus_v4.db\n",
    "blastn -num_threads 4 -query stool_sv.fasta -outfmt \"6 qacc sacc\" -db 99_otus_v4.db -max_target_seqs 1 -out stool_sv_map.blast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import biom\n",
    "from numpy.random import choice\n",
    "import skbio.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stool_sv = biom.load_table('stool_sv.biom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stool_sv_map = {}\n",
    "with open('stool_sv_map.blast') as blast_results:\n",
    "    blast_reader = csv.reader(blast_results, csv.excel_tab)\n",
    "    for row in blast_reader:\n",
    "        assert row[0].endswith('blast_rocks')\n",
    "        sv = row[0][:-len('blast_rocks')]\n",
    "        if sv in stool_sv_map:\n",
    "            assert stool_sv_map[sv] == row[1],\\\n",
    "                ' '.join([sv, stool_sv_map[sv], row[1]])\n",
    "            continue\n",
    "        stool_sv_map[sv] = row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_seqs = {}\n",
    "with open('99_otus_v4.fasta') as ref_fh:\n",
    "    fasta_reader = skbio.io.read(ref_fh, 'fasta')\n",
    "    for seq in fasta_reader:\n",
    "        ref_seqs[seq.metadata['id']] = str(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156 x 1 <class 'biom.table.Table'> with 156 nonzero entries (100% dense)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sample = stool_sv.ids()[choice(stool_sv.length())]\n",
    "random_sample = stool_sv.filter([random_sample], inplace=False)\n",
    "random_sample.filter(lambda v, _, __: v[0] > 1e-9, axis='observation', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abundance.fasta', 'w') as a_fh:\n",
    "    for row in random_sample.iter(axis='observation'):\n",
    "        abundance, sv, _ = row\n",
    "        abundance = int(abundance[0])\n",
    "        if sv in stool_sv_map:\n",
    "            a_fh.write('>' + sv + ';size=' + str(abundance) + '\\n')\n",
    "            a_fh.write(ref_seqs[stool_sv_map[sv]] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ====================ART====================\n",
      "             ART_Illumina (2008-2016)          \n",
      "          Q Version 2.5.8 (June 6, 2016)       \n",
      "     Contact: Weichun Huang <whduke@gmail.com> \n",
      "    -------------------------------------------\n",
      "\n",
      "              Amplicon 5'-end sequencing simulation\n",
      "\n",
      "Total CPU time used: 0.651425\n",
      "\n",
      "The random seed for the run: 1521688503\n",
      "\n",
      "Parameters used during run\n",
      "\tRead Length:\t150\n",
      "\tGenome masking 'N' cutoff frequency: \t1 in 150\n",
      "\t# Reads per Amplion:       0\n",
      "\tProfile Type:             Combined\n",
      "\tID Tag:                   \n",
      "\n",
      "Quality Profile(s)\n",
      "\tFirst Read:   HiSeq 2500 Length 150 R1 (built-in profile) \n",
      "\n",
      "Output files\n",
      "\n",
      "  FASTQ Sequence File:\n",
      "\tpost_art.fq\n",
      "\n",
      "Extracted to tmp/49e52d61-fe28-43c7-84b8-dfb683cb284a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_macos_x86_64, 16.0GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Rereplicating 100%\n",
      "Rereplicated 15620 reads from 156 amplicons\n",
      "Warning: your simulation will not output any ALN or SAM file with your parameter settings!\n",
      "mkdir: tmp: File exists\n",
      "mkdir: post_art: File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "vsearch --rereplicate abundance.fasta --output prior_art.fasta\n",
    "export PATH=$PATH:art_bin_MountRainier\n",
    "art_illumina -ss HS25 -amp -i prior_art.fasta -l 150 -o post_art -c 1 -na\n",
    "cat > post_art.manifest << END\n",
    "sample-id,absolute-filepath,direction\n",
    "sample-1,$PWD/post_art.fq,forward\n",
    "END\n",
    "qiime tools import --input-path post_art.manifest --output-path post_art.qza --type SampleData[SequencesWithQuality] --source-format SingleEndFastqManifestPhred33\n",
    "mkdir tmp\n",
    "qiime tools extract --output-dir tmp post_art.qza\n",
    "mkdir post_art\n",
    "mv tmp/*/data/* post_art\n",
    "rm -r tmp/*\n",
    "#qiime dada2 denoise-single --i-demultiplexed-seqs post_art.qza --o-representative-sequences dada2-seqs.qza --o-table simulated.qza --p-trunc-len 0 --p-n-threads 4 --p-no-hashed-feature-ids\n",
    "#qiime deblur denoise-16S --i-demultiplexed-seqs post_art.qza --o-representative-sequences dada2-seqs.qza --o-table simulated.qza --p-trim-length -1 --p-sample-stats --o-stats stats.qza --p-jobs-to-start 4 --p-no-hashed-feature-ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nasty\n",
    "do not run - contains R code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dir = \"post_art\"\n",
    "out_path = \"post_dada2.tsv\"\n",
    "filtered_dir = \"tmp\"\n",
    "truncLen = 150\n",
    "trimLeft = 0\n",
    "maxEE = 2.0\n",
    "truncQ = 2\n",
    "chimeraMethod = \"none\"\n",
    "minParentFold = 1.0\n",
    "nthreads = 4\n",
    "nreads_learn = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Rcpp', 'tools', 'stats', 'graphics', 'grDevices', 'utils',\n",
       "       'datasets', 'methods', 'base'], \n",
       "      dtype='<U9')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%R suppressWarnings(library(Rcpp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R version 3.4.1 (2017-06-30) \n",
       "DADA2 R package version: 1.6.0 \n",
       "1) Filtering .\n",
       "2) Learning Error Rates\n",
       "Initializing error rates to maximum possible estimate.\n",
       "Sample 1 - 15620 reads in 1745 unique sequences.\n",
       "   selfConsist step 2 \n",
       "   selfConsist step 3 \n",
       "Convergence after  3  rounds.\n",
       "\n",
       "3) Denoise remaining samples \n",
       "4) Remove chimeras (method = none)\n",
       "5) Report read numbers through the pipeline\n",
       "6) Write output\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i inp_dir,out_path,filtered_dir,truncLen,trimLeft,maxEE,truncQ,chimeraMethod,minParentFold,nthreads,nreads_learn\n",
    "#!/usr/bin/env Rscript\n",
    "\n",
    "###################################################\n",
    "# This R script takes an input directory of .fastq.gz files\n",
    "# and outputs a tsv file of the dada2 processed sequence\n",
    "# table. It is intended for use with the QIIME2 plugin\n",
    "# for DADA2.\n",
    "#\n",
    "# Ex: Rscript run_dada_single.R input_dir output.tsv filtered_dir 200 0 2.0 2 pooled 1.0 0 1000000\n",
    "####################################################\n",
    "\n",
    "####################################################\n",
    "#             DESCRIPTION OF ARGUMENTS             #\n",
    "####################################################\n",
    "# NOTE: All numeric arguments should be zero or positive.\n",
    "# NOTE: All numeric arguments save maxEE are expected to be integers.\n",
    "# NOTE: Currently the filterered_dir must already exist.\n",
    "# NOTE: ALL ARGUMENTS ARE POSITIONAL!\n",
    "#\n",
    "### FILE SYSTEM ARGUMENTS ###\n",
    "#\n",
    "# 1) File path to directory with the .fastq.gz files to be processed.\n",
    "#    Ex: path/to/dir/with/fastqgzs\n",
    "#\n",
    "# 2) File path to output tsv file. If already exists, will be overwritten.\n",
    "#    Ex: path/to/output_file.tsv\n",
    "#\n",
    "# 3) File path to directory in which to write the filtered .fastq.gz files. These files are intermediate\n",
    "#               for the full workflow. Currently they remain after the script finishes.\n",
    "#               Directory must already exist.\n",
    "#    Ex: path/to/dir/with/fastqgzs/filtered\n",
    "#\n",
    "### FILTERING ARGUMENTS ###\n",
    "#\n",
    "# 4) truncLen - The position at which to truncate reads. Reads shorter\n",
    "#               than truncLen will be discarded.\n",
    "#               Special values: 0 - no truncation or length filtering.\n",
    "#    Ex: 150\n",
    "#\n",
    "# 5) trimLeft - The number of nucleotides to remove from the start of\n",
    "#               each read. Should be less than truncLen for obvious reasons.\n",
    "#    Ex: 0\n",
    "#\n",
    "# 6) maxEE - Reads with expected errors higher than maxEE are discarded.\n",
    "#    Ex: 2.0\n",
    "#\n",
    "# 7) truncQ - Reads are truncated at the first instance of quality score truncQ.\n",
    "#                If the read is then shorter than truncLen, it is discarded.\n",
    "#    Ex: 2\n",
    "#\n",
    "### CHIMERA ARGUMENTS ###\n",
    "#\n",
    "# 8) chimeraMethod - The method used to remove chimeras. Valid options are:\n",
    "#               none: No chimera removal is performed.\n",
    "#               pooled: All reads are pooled prior to chimera detection.\n",
    "#               consensus: Chimeras are detect in samples individually, and a consensus decision\n",
    "#                           is made for each sequence variant.\n",
    "#    Ex: consensus\n",
    "#\n",
    "# 9) minParentFold - The minimum abundance of potential \"parents\" of a sequence being\n",
    "#               tested as chimeric, expressed as a fold-change versus the abundance of the sequence being\n",
    "#               tested. Values should be greater than or equal to 1 (i.e. parents should be more\n",
    "#               abundant than the sequence being tested).\n",
    "#    Ex: 1.0\n",
    "#\n",
    "### SPEED ARGUMENTS ###\n",
    "#\n",
    "# 10) nthreads - The number of threads to use.\n",
    "#                 Special values: 0 - detect available cores and use all.\n",
    "#    Ex: 1\n",
    "#\n",
    "# 11) nreads_learn - The minimum number of reads to learn the error model from.\n",
    "#                 Special values: 0 - Use all input reads.\n",
    "#    Ex: 1000000\n",
    "#\n",
    "\n",
    "cat(R.version$version.string, \"\\n\")\n",
    "\n",
    "# Input these above\n",
    "# args <- commandArgs(TRUE)\n",
    "# inp_dir <- args[[1]]\n",
    "# out_path <- args[[2]]\n",
    "# filtered_dir <- args[[3]]\n",
    "# truncLen <- as.integer(args[[4]])\n",
    "# trimLeft <- as.integer(args[[5]])\n",
    "# maxEE <- as.numeric(args[[6]])\n",
    "# truncQ <- as.integer(args[[7]])\n",
    "# chimeraMethod <- args[[8]]\n",
    "# minParentFold <- as.numeric(args[[9]])\n",
    "# nthreads <- as.integer(args[[10]])\n",
    "# nreads_learn <- as.integer(args[[11]])\n",
    "errQuit <- function(mesg, status=1) {\n",
    "  message(\"Error: \", mesg)\n",
    "  q(status=status)\n",
    "}\n",
    "\n",
    "### VALIDATE ARGUMENTS ###\n",
    "\n",
    "# Input directory is expected to contain .fastq.gz file(s)\n",
    "# that have not yet been filtered and globally trimmed\n",
    "# to the same length.\n",
    "if(!dir.exists(inp_dir)) {\n",
    "  errQuit(\"Input directory does not exist.\")\n",
    "} else {\n",
    "  unfilts <- list.files(inp_dir, pattern=\".fastq.gz$\", full.names=TRUE)\n",
    "  if(length(unfilts) == 0) {\n",
    "    errQuit(\"No input files with the expected filename format found.\")\n",
    "  }\n",
    "}\n",
    "\n",
    "# Output path is to be a filename (not a directory) and is to be\n",
    "# removed and replaced if already present.\n",
    "if(dir.exists(out_path)) {\n",
    "  errQuit(\"Output filename is a directory.\")\n",
    "} else if(file.exists(out_path)) {\n",
    "  invisible(file.remove(out_path))\n",
    "}\n",
    "\n",
    "# Convert nthreads to the logical/numeric expected by dada2\n",
    "if(nthreads < 0) {\n",
    "  errQuit(\"nthreads must be non-negative.\")\n",
    "} else if(nthreads == 0) {\n",
    "  multithread <- TRUE # detect and use all\n",
    "} else if(nthreads == 1) {\n",
    "  multithread <- FALSE\n",
    "} else {\n",
    "  multithread <- nthreads\n",
    "}\n",
    "\n",
    "### LOAD LIBRARIES ###\n",
    "suppressWarnings(library(methods))\n",
    "suppressWarnings(library(dada2))\n",
    "cat(\"DADA2 R package version:\", as.character(packageVersion(\"dada2\")), \"\\n\")\n",
    "\n",
    "### TRIM AND FILTER ###\n",
    "cat(\"1) Filtering \")\n",
    "filts <- file.path(filtered_dir, basename(unfilts))\n",
    "out <- suppressWarnings(filterAndTrim(unfilts, filts, truncLen=truncLen, trimLeft=trimLeft,\n",
    "                                      maxEE=maxEE, truncQ=truncQ, rm.phix=TRUE, \n",
    "                                      multithread=multithread))\n",
    "cat(ifelse(file.exists(filts), \".\", \"x\"), sep=\"\")\n",
    "filts <- list.files(filtered_dir, pattern=\".fastq.gz$\", full.names=TRUE)\n",
    "cat(\"\\n\")\n",
    "if(length(filts) == 0) { # All reads were filtered out\n",
    "  errQuit(\"No reads passed the filter (was truncLen longer than the read length?)\", status=2)\n",
    "}\n",
    "\n",
    "### LEARN ERROR RATES ###\n",
    "# Dereplicate enough samples to get nreads_learn total reads\n",
    "cat(\"2) Learning Error Rates\\n\")\n",
    "NREADS <- 0\n",
    "drps <- vector(\"list\", length(filts))\n",
    "for(i in seq_along(filts)) {\n",
    "  drps[[i]] <- derepFastq(filts[[i]])\n",
    "  NREADS <- NREADS + sum(drps[[i]]$uniques)\n",
    "  if(NREADS > nreads_learn) { break }\n",
    "}\n",
    "# Run dada in self-consist mode on those samples\n",
    "dds <- vector(\"list\", length(filts))\n",
    "if(i==1) { # breaks list assignment\n",
    "  dds[[1]] <- dada(drps[[1]], err=NULL, selfConsist=TRUE, multithread=multithread, VECTORIZED_ALIGNMENT=FALSE, SSE=1)\n",
    "} else { # more than one sample, no problem with list assignment\n",
    "  dds[1:i] <- dada(drps[1:i], err=NULL, selfConsist=TRUE, multithread=multithread, VECTORIZED_ALIGNMENT=FALSE, SSE=1)\n",
    "}\n",
    "err <- dds[[1]]$err_out\n",
    "rm(drps)\n",
    "cat(\"\\n\")\n",
    "\n",
    "### PROCESS ALL SAMPLES ###\n",
    "# Loop over rest in streaming fashion with learned error rates\n",
    "cat(\"3) Denoise remaining samples \")\n",
    "if(i < length(filts)) {\n",
    "  for(j in seq(i+1,length(filts))) {\n",
    "    drp <- derepFastq(filts[[j]])\n",
    "    { sink(\"/dev/null\"); dds[[j]] <- dada(drp, err=err, multithread=multithread, VECTORIZED_ALIGNMENT=FALSE, SSE=1); sink(); }\n",
    "    cat(\".\")\n",
    "  }\n",
    "}\n",
    "cat(\"\\n\")\n",
    "\n",
    "# Make sequence table\n",
    "seqtab <- makeSequenceTable(dds)\n",
    "\n",
    "### Remove chimeras\n",
    "cat(\"4) Remove chimeras (method = \", chimeraMethod, \")\\n\", sep=\"\")\n",
    "if(chimeraMethod %in% c(\"pooled\", \"consensus\")) {\n",
    "  seqtab.nochim <- removeBimeraDenovo(seqtab, method=chimeraMethod, minFoldParentOverAbundance=minParentFold, multithread=multithread)\n",
    "} else { # No chimera removal, copy seqtab to seqtab.nochim\n",
    "  seqtab.nochim <- seqtab\n",
    "}\n",
    "\n",
    "### REPORT READ FRACTIONS THROUGH PIPELINE ###\n",
    "cat(\"5) Report read numbers through the pipeline\\n\")\n",
    "# Handle edge cases: Samples lost in filtering; One sample\n",
    "track <- cbind(out, matrix(0, nrow=nrow(out), ncol=2))\n",
    "colnames(track) <- c(\"input\", \"filtered\", \"denoised\", \"non-chimeric\")\n",
    "passed.filtering <- track[,\"filtered\"] > 0\n",
    "track[passed.filtering,\"denoised\"] <- rowSums(seqtab)\n",
    "track[passed.filtering,\"non-chimeric\"] <- rowSums(seqtab.nochim)\n",
    "head(track)\n",
    "#write.table(track, out.track, sep=\"\\t\",\n",
    "#            row.names=TRUE, col.names=col.names, quote=FALSE)\n",
    "\n",
    "### WRITE OUTPUT AND QUIT ###\n",
    "# Formatting as tsv plain-text sequence table table\n",
    "cat(\"6) Write output\\n\")\n",
    "seqtab.nochim <- t(seqtab.nochim) # QIIME has OTUs as rows\n",
    "col.names <- basename(filts)\n",
    "col.names[[1]] <- paste0(\"#OTU ID\\t\", col.names[[1]])\n",
    "write.table(seqtab.nochim, out_path, sep=\"\\t\",\n",
    "            row.names=TRUE, col.names=col.names, quote=FALSE)\n",
    "saveRDS(seqtab.nochim, gsub(\"tsv\", \"rds\", out_path)) ### TESTING\n",
    "\n",
    "# Clearly not required in a jupyter cell\n",
    "# q(status=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 1 45 1 1 1 43 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 28 26 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 29 41 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 30 16 1 1 1 31 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 42 7 8 37 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 39 32 33 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 47 38 34 1 35 31 20 1 1 2 2 2 2 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 44 51 13 13 5 40 46 49 36 52 2 1 2 2 4 9 6 6 9 4 4 4 2 2 2 2 2 2 2 2 6 2 2 2 2 2 3 3 3 3 3 3 3 3 4 3 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 48 5 13 22 41 47 22 51 5 18 5 5 5 5 5 16 41 18 49 7 7 28 50 1 1 11 25 25 8 8 10 10 26 41 5 13 1 1 5 1 27 35 35 35 8 1 6 1 1 7 2 2 3 1 1 5 2 31 31 12 12 1 15 12 12 15 15 15 15 15 15 15 15 15 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 1 12 15 15 15 15 12 12 20 20 20 20 20 20 20 20 20 20 20 14 2 1 1 8 21 3 4 3 1 1 1 2 3 14 1 1 2 3 4 3 6 4 3 3 6 3 6 4 2 2 3 3 6 2 9 2 9 2 9 3 4 4 3 2 3 6 2 3 3 3 4 3 9 3 4 3 9 3 2 6 2 3 6 2 2 9 2 2 6 6 9 2 2 9 4 4 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 2 9 4 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 9 6 2 2 9 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 2 3 3 2 3 3 3 3 6 2 2 3 3 9 6 2 4 3 3 3 1 1 3 3 4 2 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 14 1 1 1 1 1 1 14 1 1 1 1 1 14 1 1 1 1 1 1 1 1 1 14 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 14 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 14 1 1 1 1 1 14 1 14 14 14 14 14 1 14 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 14 1 1 1 1 1 1 1 1 1 14 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 14 1 1 1 1 1 1 1 1 6 3 1 14 1 1 2 1 2 1 1 1 8 5 7 46 11 24 24 24 24 24 24 24 29 29 29 29 29 41 7 5 5 5 13 18 7 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 7 18 5 7 5 7 5 18 42 7 7 41 7 42 42 42 27 47 28 28 28 28 28 27 22 5 18 18 18 18 18 18 18 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 16 16 16 16 16 45 45 45 45 41 18 41 41 41 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 41 7 5 41 5 16 7 18 18 22 22 22 22 22 22 13 16 5 7 18 16 27 45 5 5 7 7 5 7 13 5 7 13 23 23 23 16 39 39 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 8 32 8 33 33 38 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 25 25 25 25 49 25 25 25 25 25 25 34 21 21 21 43 43 1 1 11 11 17 43 10 8 10 10 10 10 43 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 1 1 10 17 17 17 17 17 17 17 17 17 17 17 17 17 17 10 8 17 5 26 37 26 37 37 37 7 19 19 19 19 19 19 26 26 42 28 28 16 26 26 26 42 26 26 26 43 7 1 1 1 1 1 1 13 7 4 3 1 5 7 4 1 1 1 24 11 34 3 1 7 11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "cat(dds[[1]]$map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
