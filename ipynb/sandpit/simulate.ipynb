{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "- Create a notebook to download and preprocess that summarises the steps\n",
    " - preprocessing includes defining folds\n",
    "- Create a notebook generate weights and train classifiers\n",
    "- Create a command-line script for generating expected/frequency/sequence triples from a biom and a sv_map\n",
    "- Create a command-line script for generating observeds for\n",
    " - uniform weights\n",
    " - global weights\n",
    " - bespoke weights\n",
    " - wrong weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n",
    "download stool SVs with abundances\n",
    "\n",
    "do not run - takes hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "redbiom search metadata 'where sample_type == \"stool\"' > stool_samples\n",
    "redbiom search metadata 'where sample_type == \"Stool\"' >> stool_samples\n",
    "export CTX=Deblur-illumina-16S-v4-150nt-10d7e0\n",
    "redbiom fetch samples --from stool_samples --context $CTX --output stool_sv.biom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "extract V4 for 99% greengenes\n",
    "\n",
    "blast the stool SVs against the greengenes amplicons\n",
    "\n",
    "do not run - takes overnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime tools import --input-path 99_otus.fasta --output-path 99_otus.qza --type FeatureData[Sequence]\n",
    "qiime feature-classifier extract-reads --i-sequences 99_otus.qza --p-f-primer GTGYCAGCMGCCGCGGTAA --p-r-primer GGACTACNVGGGTWTCTAAT --o-reads 99_otus_v4.qza\n",
    "qiime tools export 99_otus_v4.qza --output-dir .\n",
    "mv dna-sequences.fasta 99_otus_v4.fasta\n",
    "biom table-ids --observations -i stool_sv.biom | awk '{print \">\"$1\"blast_rocks\\n\"$1}' > stool_sv.fasta\n",
    "makeblastdb -in 99_otus_v4.fasta -dbtype nucl -out 99_otus_v4.db\n",
    "blastn -num_threads 4 -query stool_sv.fasta -outfmt \"6 qacc sacc\" -db 99_otus_v4.db -max_target_seqs 1 -out stool_sv_map.blast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "import hashlib\n",
    "\n",
    "import biom\n",
    "from numpy.random import choice\n",
    "import skbio.io\n",
    "from pandas import DataFrame, Series\n",
    "from qiime2 import Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stool_sv = biom.load_table('stool_sv.biom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct a mapping from each SV to a sequence label from 99% greengenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stool_sv_map = {}\n",
    "with open('stool_sv_map.blast') as blast_results:\n",
    "    blast_reader = csv.reader(blast_results, csv.excel_tab)\n",
    "    for row in blast_reader:\n",
    "        assert row[0].endswith('blast_rocks')\n",
    "        sv = row[0][:-len('blast_rocks')]\n",
    "        if sv in stool_sv_map:\n",
    "            assert stool_sv_map[sv] == row[1],\\\n",
    "                ' '.join([sv, stool_sv_map[sv], row[1]])\n",
    "            continue\n",
    "        stool_sv_map[sv] = row[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct a mapping from each sequence label to its amplicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('99_otus_v4.fasta') as ref_fh:\n",
    "    fasta_reader = skbio.io.read(ref_fh, 'fasta')\n",
    "    ref_seqs = {s.metadata['id']: str(s) for s in fasta_reader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct a mapping from each sequence label to its taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('99_otu_taxonomy.txt') as tax_fh:\n",
    "    tax_reader = csv.reader(tax_fh, csv.excel_tab)\n",
    "    tax_map = {r[0]: r[1] for r in tax_reader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose a random stool sample, extract it, and filter out SVs with zero abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389 x 1 <class 'biom.table.Table'> with 389 nonzero entries (100% dense)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sample = stool_sv.ids()[choice(stool_sv.length())]\n",
    "random_sample = stool_sv.filter([random_sample], inplace=False)\n",
    "random_sample.filter(lambda v, _, __: v[0] > 1e-9, axis='observation', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output the amplicon sequences to fasta, labelled by greengenes sequence label, with abundance that `vsearch --rereplicate` will understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abundance.fasta', 'w') as a_fh:\n",
    "    for row in random_sample.iter(axis='observation'):\n",
    "        abundance, sv, _ = row\n",
    "        abundance = int(abundance[0])\n",
    "        if sv in stool_sv_map:\n",
    "            label = stool_sv_map[sv]\n",
    "            a_fh.write('>' + label + ';size=' + str(abundance) + '\\n')\n",
    "            a_fh.write(ref_seqs[stool_sv_map[sv]] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repreplicate according to abundance and run ART to simulate amplicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ====================ART====================\n",
      "             ART_Illumina (2008-2016)          \n",
      "          Q Version 2.5.8 (June 6, 2016)       \n",
      "     Contact: Weichun Huang <whduke@gmail.com> \n",
      "    -------------------------------------------\n",
      "\n",
      "              Amplicon 5'-end sequencing simulation\n",
      "\n",
      "Total CPU time used: 2.34203\n",
      "\n",
      "The random seed for the run: 1523591680\n",
      "\n",
      "Parameters used during run\n",
      "\tRead Length:\t150\n",
      "\tGenome masking 'N' cutoff frequency: \t1 in 150\n",
      "\t# Reads per Amplion:       0\n",
      "\tProfile Type:             Combined\n",
      "\tID Tag:                   \n",
      "\n",
      "Quality Profile(s)\n",
      "\tFirst Read:   HiSeq 2500 Length 150 R1 (built-in profile) \n",
      "\n",
      "Output files\n",
      "\n",
      "  FASTQ Sequence File:\n",
      "\tpost_art.fq\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.7.0_macos_x86_64, 16.0GB RAM, 8 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Rereplicating 100%\n",
      "Rereplicated 55811 reads from 389 amplicons\n",
      "Warning: your simulation will not output any ALN or SAM file with your parameter settings!\n",
      "rm: dada_tmp: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "vsearch --rereplicate abundance.fasta --output prior_art.fasta\n",
    "export PATH=$PATH:art_bin_MountRainier\n",
    "art_illumina -ss HS25 -amp -i prior_art.fasta -l 150 -o post_art -c 1 -na\n",
    "if [ -d dada_in ]; then\n",
    "    rm -r dada_in\n",
    "    rm -r dada_tmp\n",
    "    rm -r dada_out\n",
    "fi\n",
    "mkdir dada_in\n",
    "mkdir dada_out\n",
    "gzip post_art.fq\n",
    "mv post_art.fq.gz dada_in/post_art.fastq.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoise\n",
    "do not run - contains R code\n",
    "```\n",
    "inp_dir = \"dada_in\"\n",
    "out_path = \"post_dada2.tsv\"\n",
    "filtered_dir = \"dada_tmp\"\n",
    "truncLen = 150\n",
    "trimLeft = 0\n",
    "maxEE = 2.0\n",
    "truncQ = 2\n",
    "chimeraMethod = \"none\"\n",
    "minParentFold = 1.0\n",
    "nthreads = 4\n",
    "nreads_learn = 1000000\n",
    "trace_dir = \"dada_out\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_traceable_dada_single.R\n"
     ]
    }
   ],
   "source": [
    "%%file run_traceable_dada_single.R\n",
    "#!/usr/bin/env Rscript\n",
    "\n",
    "###################################################\n",
    "# This R script takes an input directory of .fastq.gz files\n",
    "# and outputs a tsv file of the dada2 processed sequence\n",
    "# table. It is intended for use with the QIIME2 plugin\n",
    "# for DADA2.\n",
    "#\n",
    "# Ex: Rscript run_dada_single.R input_dir output.tsv filtered_dir 200 0 2.0 2 pooled 1.0 0 1000000\n",
    "####################################################\n",
    "\n",
    "####################################################\n",
    "#             DESCRIPTION OF ARGUMENTS             #\n",
    "####################################################\n",
    "# NOTE: All numeric arguments should be zero or positive.\n",
    "# NOTE: All numeric arguments save maxEE are expected to be integers.\n",
    "# NOTE: Currently the filterered_dir must already exist.\n",
    "# NOTE: ALL ARGUMENTS ARE POSITIONAL!\n",
    "#\n",
    "### FILE SYSTEM ARGUMENTS ###\n",
    "#\n",
    "# 1) File path to directory with the .fastq.gz files to be processed.\n",
    "#    Ex: path/to/dir/with/fastqgzs\n",
    "#\n",
    "# 2) File path to output tsv file. If already exists, will be overwritten.\n",
    "#    Ex: path/to/output_file.tsv\n",
    "#\n",
    "# 3) File path to directory in which to write the filtered .fastq.gz files. These files are intermediate\n",
    "#               for the full workflow. Currently they remain after the script finishes.\n",
    "#               Directory must already exist.\n",
    "#    Ex: path/to/dir/with/fastqgzs/filtered\n",
    "#\n",
    "### FILTERING ARGUMENTS ###\n",
    "#\n",
    "# 4) truncLen - The position at which to truncate reads. Reads shorter\n",
    "#               than truncLen will be discarded.\n",
    "#               Special values: 0 - no truncation or length filtering.\n",
    "#    Ex: 150\n",
    "#\n",
    "# 5) trimLeft - The number of nucleotides to remove from the start of\n",
    "#               each read. Should be less than truncLen for obvious reasons.\n",
    "#    Ex: 0\n",
    "#\n",
    "# 6) maxEE - Reads with expected errors higher than maxEE are discarded.\n",
    "#    Ex: 2.0\n",
    "#\n",
    "# 7) truncQ - Reads are truncated at the first instance of quality score truncQ.\n",
    "#                If the read is then shorter than truncLen, it is discarded.\n",
    "#    Ex: 2\n",
    "#\n",
    "### CHIMERA ARGUMENTS ###\n",
    "#\n",
    "# 8) chimeraMethod - The method used to remove chimeras. Valid options are:\n",
    "#               none: No chimera removal is performed.\n",
    "#               pooled: All reads are pooled prior to chimera detection.\n",
    "#               consensus: Chimeras are detect in samples individually, and a consensus decision\n",
    "#                           is made for each sequence variant.\n",
    "#    Ex: consensus\n",
    "#\n",
    "# 9) minParentFold - The minimum abundance of potential \"parents\" of a sequence being\n",
    "#               tested as chimeric, expressed as a fold-change versus the abundance of the sequence being\n",
    "#               tested. Values should be greater than or equal to 1 (i.e. parents should be more\n",
    "#               abundant than the sequence being tested).\n",
    "#    Ex: 1.0\n",
    "#\n",
    "### SPEED ARGUMENTS ###\n",
    "#\n",
    "# 10) nthreads - The number of threads to use.\n",
    "#                 Special values: 0 - detect available cores and use all.\n",
    "#    Ex: 1\n",
    "#\n",
    "# 11) nreads_learn - The minimum number of reads to learn the error model from.\n",
    "#                 Special values: 0 - Use all input reads.\n",
    "#    Ex: 1000000\n",
    "#\n",
    "\n",
    "cat(R.version$version.string, \"\\n\")\n",
    "\n",
    "args <- commandArgs(TRUE)\n",
    "inp_dir <- args[[1]]\n",
    "out_path <- args[[2]]\n",
    "filtered_dir <- args[[3]]\n",
    "truncLen <- as.integer(args[[4]])\n",
    "trimLeft <- as.integer(args[[5]])\n",
    "maxEE <- as.numeric(args[[6]])\n",
    "truncQ <- as.integer(args[[7]])\n",
    "chimeraMethod <- args[[8]]\n",
    "minParentFold <- as.numeric(args[[9]])\n",
    "nthreads <- as.integer(args[[10]])\n",
    "nreads_learn <- as.integer(args[[11]])\n",
    "trace_dir <- args[[12]]\n",
    "errQuit <- function(mesg, status=1) {\n",
    "  message(\"Error: \", mesg)\n",
    "  q(status=status)\n",
    "}\n",
    "\n",
    "### VALIDATE ARGUMENTS ###\n",
    "\n",
    "# Input directory is expected to contain .fastq.gz file(s)\n",
    "# that have not yet been filtered and globally trimmed\n",
    "# to the same length.\n",
    "if(!dir.exists(inp_dir)) {\n",
    "  errQuit(\"Input directory does not exist.\")\n",
    "} else {\n",
    "  unfilts <- list.files(inp_dir, pattern=\".fastq.gz$\", full.names=TRUE)\n",
    "  if(length(unfilts) == 0) {\n",
    "    errQuit(\"No input files with the expected filename format found.\")\n",
    "  }\n",
    "}\n",
    "\n",
    "# Output path is to be a filename (not a directory) and is to be\n",
    "# removed and replaced if already present.\n",
    "if(dir.exists(out_path)) {\n",
    "  errQuit(\"Output filename is a directory.\")\n",
    "} else if(file.exists(out_path)) {\n",
    "  invisible(file.remove(out_path))\n",
    "}\n",
    "\n",
    "# Convert nthreads to the logical/numeric expected by dada2\n",
    "if(nthreads < 0) {\n",
    "  errQuit(\"nthreads must be non-negative.\")\n",
    "} else if(nthreads == 0) {\n",
    "  multithread <- TRUE # detect and use all\n",
    "} else if(nthreads == 1) {\n",
    "  multithread <- FALSE\n",
    "} else {\n",
    "  multithread <- nthreads\n",
    "}\n",
    "\n",
    "if(!dir.exists(trace_dir)) {\n",
    "  errQuit(\"Trace directory does not exist.\")\n",
    "}\n",
    "\n",
    "### LOAD LIBRARIES ###\n",
    "suppressWarnings(library(methods))\n",
    "suppressWarnings(library(dada2))\n",
    "cat(\"DADA2 R package version:\", as.character(packageVersion(\"dada2\")), \"\\n\")\n",
    "\n",
    "### TRIM AND FILTER ###\n",
    "cat(\"1) Filtering \")\n",
    "filts <- file.path(filtered_dir, basename(unfilts))\n",
    "out <- suppressWarnings(filterAndTrim(unfilts, filts, truncLen=truncLen, trimLeft=trimLeft,\n",
    "                                      maxEE=maxEE, truncQ=truncQ, rm.phix=TRUE, \n",
    "                                      multithread=multithread))\n",
    "cat(ifelse(file.exists(filts), \".\", \"x\"), sep=\"\")\n",
    "filts <- list.files(filtered_dir, pattern=\".fastq.gz$\", full.names=TRUE)\n",
    "cat(\"\\n\")\n",
    "if(length(filts) == 0) { # All reads were filtered out\n",
    "  errQuit(\"No reads passed the filter (was truncLen longer than the read length?)\", status=2)\n",
    "}\n",
    "\n",
    "### LEARN ERROR RATES ###\n",
    "# Dereplicate enough samples to get nreads_learn total reads\n",
    "cat(\"2) Learning Error Rates\\n\")\n",
    "NREADS <- 0\n",
    "drps <- vector(\"list\", length(filts))\n",
    "for(i in seq_along(filts)) {\n",
    "  drps[[i]] <- derepFastq(filts[[i]])\n",
    "  NREADS <- NREADS + sum(drps[[i]]$uniques)\n",
    "  if(NREADS > nreads_learn) { break }\n",
    "}\n",
    "# Run dada in self-consist mode on those samples\n",
    "dds <- vector(\"list\", length(filts))\n",
    "if(i==1) { # breaks list assignment\n",
    "  dds[[1]] <- dada(drps[[1]], err=NULL, selfConsist=TRUE, multithread=multithread, VECTORIZED_ALIGNMENT=FALSE, SSE=1)\n",
    "} else { # more than one sample, no problem with list assignment\n",
    "  dds[1:i] <- dada(drps[1:i], err=NULL, selfConsist=TRUE, multithread=multithread, VECTORIZED_ALIGNMENT=FALSE, SSE=1)\n",
    "}\n",
    "err <- dds[[1]]$err_out\n",
    "# rm(drps)\n",
    "cat(\"\\n\")\n",
    "\n",
    "### PROCESS ALL SAMPLES ###\n",
    "# Loop over rest with learned error rates\n",
    "cat(\"3) Denoise remaining samples \")\n",
    "if(i < length(filts)) {\n",
    "  for(j in seq(i+1,length(filts))) {\n",
    "    drps[[j]] <- derepFastq(filts[[j]])\n",
    "    { sink(\"/dev/null\"); dds[[j]] <- dada(drps[[j]], err=err, multithread=multithread, VECTORIZED_ALIGNMENT=FALSE, SSE=1); sink(); }\n",
    "    cat(\".\")\n",
    "  }\n",
    "}\n",
    "cat(\"\\n\")\n",
    "\n",
    "for(j in seq(1, length(filts))){\n",
    "  map_path <- file.path(trace_dir, gsub('fastq.gz', 'map', basename(filts[[j]])))\n",
    "  uniques <- getSequences(drps[[j]])\n",
    "  svs <- names(dds[[j]]$denoised[unname(dds[[j]]$map)])\n",
    "  write.table(t(rbind(uniques, svs)),\n",
    "              map_path, sep=\"\\t\", quote=FALSE, row.names=FALSE, col.names=FALSE)\n",
    "}\n",
    "rm(drps)\n",
    "\n",
    "# Make sequence table\n",
    "seqtab <- makeSequenceTable(dds)\n",
    "\n",
    "### Remove chimeras\n",
    "cat(\"4) Remove chimeras (method = \", chimeraMethod, \")\\n\", sep=\"\")\n",
    "if(chimeraMethod %in% c(\"pooled\", \"consensus\")) {\n",
    "  seqtab.nochim <- removeBimeraDenovo(seqtab, method=chimeraMethod, minFoldParentOverAbundance=minParentFold, multithread=multithread)\n",
    "} else { # No chimera removal, copy seqtab to seqtab.nochim\n",
    "  seqtab.nochim <- seqtab\n",
    "}\n",
    "\n",
    "### REPORT READ FRACTIONS THROUGH PIPELINE ###\n",
    "cat(\"5) Report read numbers through the pipeline\\n\")\n",
    "# Handle edge cases: Samples lost in filtering; One sample\n",
    "track <- cbind(out, matrix(0, nrow=nrow(out), ncol=2))\n",
    "colnames(track) <- c(\"input\", \"filtered\", \"denoised\", \"non-chimeric\")\n",
    "passed.filtering <- track[,\"filtered\"] > 0\n",
    "track[passed.filtering,\"denoised\"] <- rowSums(seqtab)\n",
    "track[passed.filtering,\"non-chimeric\"] <- rowSums(seqtab.nochim)\n",
    "head(track)\n",
    "#write.table(track, out.track, sep=\"\\t\",\n",
    "#            row.names=TRUE, col.names=col.names, quote=FALSE)\n",
    "\n",
    "### WRITE OUTPUT AND QUIT ###\n",
    "# Formatting as tsv plain-text sequence table table\n",
    "cat(\"6) Write output\\n\")\n",
    "seqtab.nochim <- t(seqtab.nochim) # QIIME has OTUs as rows\n",
    "col.names <- basename(filts)\n",
    "col.names[[1]] <- paste0(\"#OTU ID\\t\", col.names[[1]])\n",
    "write.table(seqtab.nochim, out_path, sep=\"\\t\",\n",
    "            row.names=TRUE, col.names=col.names, quote=FALSE)\n",
    "#saveRDS(seqtab.nochim, gsub(\"tsv\", \"rds\", out_path)) ### TESTING\n",
    "\n",
    "q(status=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R version 3.4.1 (2017-06-30) \n",
      "Loading required package: Rcpp\n",
      "DADA2 R package version: 1.6.0 \n",
      "1) Filtering .\n",
      "2) Learning Error Rates\n",
      "Initializing error rates to maximum possible estimate.\n",
      "Sample 1 - 55811 reads in 8195 unique sequences.\n",
      "   selfConsist step 2 \n",
      "   selfConsist step 3 \n",
      "   selfConsist step 4 \n",
      "Convergence after  4  rounds.\n",
      "\n",
      "3) Denoise remaining samples \n",
      "4) Remove chimeras (method = none)\n",
      "5) Report read numbers through the pipeline\n",
      "                  input filtered denoised non-chimeric\n",
      "post_art.fastq.gz 55811    55811    55811        55811\n",
      "6) Write output\n"
     ]
    }
   ],
   "source": [
    "!Rscript run_traceable_dada_single.R dada_in post_dada2.tsv dada_tmp 150 0 2 2 none 1 4 1000000 dada_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct\n",
    "need\n",
    "- `FeatureData[Taxonomy]` for expected taxonomy\n",
    "- `FeatureTable[Frequency]` for abundance\n",
    "- `FeatureData[Sequence]` for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_map = defaultdict(list)\n",
    "with open('dada_out/post_art.map') as pam_fh:\n",
    "    pam_reader = csv.reader(pam_fh, csv.excel_tab)\n",
    "    for unique, sv in pam_reader:\n",
    "        unique_map[sv].append(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_map = defaultdict(list)\n",
    "with skbio.io.open('dada_in/post_art.fastq.gz') as pa_fh:\n",
    "    fastq_reader = skbio.io.read(pa_fh, 'fastq', phred_offset=33)\n",
    "    for seq in fastq_reader:\n",
    "        noise_map[str(seq)].append(tax_map[seq.metadata['id'][:-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Counter()\n",
    "abundance = {}\n",
    "with open('post_dada2.tsv') as pd2_fh:\n",
    "    dada_reader = csv.reader(pd2_fh, csv.excel_tab)\n",
    "    dada_reader.__next__()\n",
    "    for sv, total_abundance in dada_reader:\n",
    "        for seq in unique_map[sv]:\n",
    "            for taxon in noise_map[seq]:\n",
    "                result[(sv, taxon)] += 1\n",
    "        abundance[sv] = int(total_abundance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "check = Counter()\n",
    "for (sv, taxon), count in result.items():\n",
    "    check[sv] += count\n",
    "for sv, count in check.items():\n",
    "    assert abundance[sv] == count, sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sequences.qza'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened = [(s, t, c) for (s, t), c in result.items()]\n",
    "svs, taxa, abundances = zip(*flattened)\n",
    "hashes = [hashlib.md5((s+t).encode('utf-8')).hexdigest() for s, t in result]\n",
    "expected = DataFrame({'Taxon': taxa}, index=hashes, columns=['Taxon'])\n",
    "expected.index.name = 'Feature ID'\n",
    "expected = Artifact.import_data('FeatureData[Taxonomy]', expected)\n",
    "expected.save('expected.qza')\n",
    "abundanced = DataFrame({'Abundance': abundances}, index=hashes,\n",
    "                       columns=['Abundance'])\n",
    "abundanced = Artifact.import_data('FeatureTable[Frequency]', abundanced)\n",
    "abundanced.save('frequencies.qza')\n",
    "sequences = Series(svs, index=hashes)\n",
    "sequences = Artifact.import_data('FeatureData[Sequence]', sequences)\n",
    "sequences.save('sequences.qza')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "Ok, so we can generate a single sample. Now we have to write a script that we can run on a cluster that will do it k-foldwise. For each fold we will need\n",
    "- `FeatureData[Taxonomy]` to train to classifier\n",
    "- `FeatureTable[RelativeFrequency]` for weights to train classifier\n",
    "- `FeatureData[Sequence]` to train classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
